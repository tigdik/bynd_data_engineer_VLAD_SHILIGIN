# Bynd Data Engineering Exercise â€“ Vlad Shiligin

This repository is a **minimal, productionâ€‘ready PySpark pipeline** that ingests the provided
`mock_dataset.csv`, performs basic data cleansing, and loads the result into a Postgres table.

## Quickâ€‘start (local)

```bash
# 1. Clone and enter
git clone https://github.com/tigdik/bynd_data_engineer_VLAD_SHILIGIN.git bynd_data_engineer_VLAD_SHILIGIN
cd bynd_data_engineer_VLAD_SHILIGIN

# 2. Configure environment variables in
.env  

# 3. Start Postgres locally
docker compose up -d db

# 4. Build python env and run pipeline on host (optional)
make dev && make run

# 5. ...or run everything inside Docker
docker build -t bynd-pipeline .
docker run --env-file .env --network=host bynd-pipeline
```

## Project layout

```
.
â”œâ”€â”€ src/                    # Importable package (`bynd_de`)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py           # pydantic settings from env
â”‚   â”œâ”€â”€ spark_session.py    # single SparkSession factory
â”‚   â”œâ”€â”€ transformations.py  # stateless transform fns
â”‚   â””â”€â”€ pipeline.py         # extractâ€‘transformâ€‘load script
â”œâ”€â”€ tests/                  # PyTest unit tests
â”œâ”€â”€ data/                   # Raw mock data (gitâ€‘ignored)
â”œâ”€â”€ Dockerfile              # Builds + runs `spark-submit`
â”œâ”€â”€ docker-compose.yml      # Postgres service
â”œâ”€â”€ Makefile                # Dev shortcuts
â””â”€â”€ .github/workflows/ci.yml
```

## Design decisions

* **PySpark** as suggested by Dinarte.
* **Stateless functions** in `transformations.py` â€“ easily unitâ€‘tested & reused.
* **Configuration via env vars** â€“ no need to recompile if the need to change settings raised.
* **CI** runs tests, lint (`ruff`), typeâ€‘check (`mypy`) on every push.
* **Security** â€“ leastâ€‘privilege Postgres user, secrets not baked into images.

## Extending

* Add richer parsing (e.g. JSON strings in `address` or currency parsing) by
  creating new functions in `transformations.py` and adding them to
  `transform()` composition.
* Swap sinks (e.g. S3, BigQuery) by implementing a new `Writer` module that
  consumes the final `DataFrame`.

---

*Happy Engineering!* ðŸš€
